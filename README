

FOR RUNNING THE PROJECT:
1. install node js
from https://nodejs.org/en

2. in terminal at the project root directory,
 run the command (below command would install all the dependencies i.e libraries required by the project mentioned in package.json)
npm install

3. for running the project, in terminal at the project root directory,
run the command: (the below command would run the index.js file)
npm run start

FOR ACCESSING DATA/EXPORTING TO MY SQL QUERIES OR CSV FILE:
1. download TablePlus (this is a db client i.e which lets u connect to a db server)

2. make a new connection of sqlite, use the file plants-sqlite.db as the db source
(Since I have set up the db as sqlite, it's actually not a server, but just a file: plants-sqlite.db)

INDEX FILE:
runIndexScrapperJobs();
in index file is commented out as all the index pages are scrapped and urls from those pages
are put in db table scrapper_jobs.

the reason for decoupling the index pages and plant pages is: sometimes in scrapping random error occurs
during scrapping of a page, and script crashes. So in those cases, all the stuff gets started from zero i.e
we wouldnt want the index pages to be scrapped again.

runPageScrapperJobs();
this function would pull the jobs from scrapper_jobs table in db, and run the plant page scrapping

Also when a page is successfully scrapped, its state is marked done (done column in the scrapper_jobs table )
so in case of error (exception) that done wont be marked as 1 (and it would remain 0).

Hence, later on i can always debug the failing pages.

generateCSV();

this function generates a csv containing all the scrapped plants pages
Once all the plants pages are scrapped, you will have a csv.

CONFIG-PROPERTIES:
MIN_THREADS and MAX_THREADS configure how many parallel scrapping jobs you want to run

